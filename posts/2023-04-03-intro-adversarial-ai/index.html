<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Introduction to Adversarial AI - Blog</title><meta name=description content="The following is a transcription of a talk I&rsquo;ve given internally at GitHub. The talk, and slides, are about the basics of adversarial AI, and how it can be used to attack machine learning models.
Slides 
Transcript How do Machine Learning Systems work? For the purposes of this talk/ blog post, we’re not going to go too in depth into the technical details of machine learning algorithms. However, from a security perspective, it is important to note the flow of data."><meta name=author content="Rahul Zhade"><link href=https://unpkg.com/@master/normal.css rel=stylesheet><script src=https://unpkg.com/@master/style@1.5.0></script>
<script src=https://unpkg.com/@master/styles@1.13.0></script>
<script src=https://unpkg.com/master-styles-group></script>
<script src=https://unpkg.com/themes.js></script>
<script>window.themes=window.themes||new window.Themes</script><style>:root{--font-sans:"Inter var", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji}</style></head><body class="bg:fade-84@dark font:fade-16@dark font:sans"><nav class="w:full h:90 fixed bg:fade-84/.95@dark bg:white z:1000"><div class="h:full
w:full
max-w:1200
mx:auto
px:32
d:flex
align-items:center"><div><a href=/ class="mr-3 font:extralight">Blog</a></div><div class=ml:auto></div></div></nav><div class="d:flex flex:column@<=sm pt:90 px:24 jc:center gap:44 word-break:break-word"><div class="max-w:700 w:full box:content-box"><article class="box:border-box pt:32"><header class=mb:32><div class="font:40 font:extrabold">Introduction to Adversarial AI</div><div class="mt:16 f:fade-60"><time>Apr 3, 2023</time></div></header><div class="_:where(a):hover{text-decoration-color:fade}
_:where(a){text-decoration:2;underline;fade-10;_text-decoration-color:fade-70@dark}
_:where(blockquote){bl:5;solid;fade-76/.1;_bl:5;solid;fade-34/.1@dark}
_:where(code){font:90%;_v:middle}
_:where(code:not(.highlight_*,pre_*)){p:2;6;_r:4}
_:where(del){text-decoration:1;line-through;fade-68;_text-decoration-color:red-64@dark}
_:where(figcaption){text:14;_p:10;20;0;_width:fit;_mx:auto;_font:fade-56;_font:fade-57@dark}
_:where(h1){font:40;_font:extrabold}
_:where(h1,h2,h3)+:where(h1,h2,h3){mt:.5em}
_:where(h1,h2,h3,h4,h5,h6){mt:2em}
_:where(h2){mb:1em;_font:32}
_:where(h3){font:24}
_:where(h4){font:20}
_:where(h5){font:16}
_:where(h6){font:14}
_:where(li)::marker{font:fade-44;_font:fade-68@dark}
_:where(li){pl:.375em}
_:where(mark){text-decoration:1;underline;#fce016;_bg:transparent;_text-decoration-color:rgb(252;224;22/.5)@dark}
_:where(p,li){font:fade-76;_font:16;_line-height:1.65;_font:fade-34@dark}
_:where(p,pre,blockquote,figure,ul,ol,table){my:1.125em}
>:first-child{mt:0!}
_:where(pre){p:20;_r:8;_overflow:auto}
_:where(pre,code:not(.highlight_*)){bg:fade-2;_bg:fade-92!@dark}
_:where(strong,b,a,code:not(.highlight_*),mark,del){font:fade-92;_font:fade-12@dark}
_:where(table){width:full;_border-spacing:0}
_:where(td){v:baseline}
_:where(td,th):first-child{pl:0}
_:where(td,th):last-child{pr:0}
_:where(td,th){bb:1;solid;fade-92/.06;_p:6;_b:fade-4/.04@dark}
_:where(th){font:fade-78;_font:14;_text:left;_font:fade-12@dark}
_:where(th,p_code,li_code,a,mark){font:semibold;_font:medium@dark}
_:where(ul){list-style-type:disc}
_:where(ul,ol,blockquote){pl:1.5em}
_:where(video,img){max-width:full}
_:where(a,mark){text-underline-offset:3}
_:where(hr){h:2;_bg:fade-10;_bg:fade-70@dark;_my:3em}"><p>The following is a transcription of a talk I&rsquo;ve given internally at GitHub. The talk, and <a href=https://zhade.dev/slides/presentations/adversarial-ai>slides</a>, are about the basics of adversarial AI, and how it can be used to attack machine learning models.</p><h2 id=slides>Slides</h2><p><a href=https://zhade.dev/slides/presentations/adversarial-ai><iframe src=https://zhade.dev/slides/presentations/adversarial-ai width=100%></iframe></a></p><h2 id=transcript>Transcript</h2><h3 id=how-do-machine-learning-systems-work>How do Machine Learning Systems work?</h3><p>For the purposes of this talk/ blog post, we’re not going to go too in depth into the technical details of machine learning algorithms. However, from a security perspective, it is important to note the flow of data.</p><p><img src=./images/ml-system-flowchart.png alt="Machine Learning System Flowchart"></p><ol><li>We start with some raw data. This data can be sourced from any number of places, but typically can be from the Internet, or some internal data source.</li><li>This data is then <em>cleaned</em> and <em>preprocessed</em> to make it usable by the model. This can include things like removing punctuation, normalizing the data, or even removing data that is not relevant to the model.</li><li>This data is then split into two sets, the <em>training</em> set, and the <em>testing</em> set. The training set is used to train the model, and the testing set is used to evaluate the model.</li><li>The model is then trained on the training set. This is where the actual machine learning happens. The model is trained to recognize patterns in the data, and make predictions based on those patterns.</li><li>The model is then evaluated on the testing set. This is where we can see how well the model performs on data it has never seen before. This is important because it allows us to see how well the model generalizes to new data.</li><li>Finally, when we are satisfied with the performance of the model on the testing set, we deploy it to production.</li></ol><p>This is highly abbreviated, however the important thing to note here is the number of moving parts in this system. Any system with a large number of moving parts and trust boundaries is very interesting from a security perspective.</p><h3 id=what-is-application-security>What is Application Security?</h3><p>Application Security is a discipline that ensures the security of software applications from security misuses. The main three classes that these kinds of misuses are classified into are: <strong>Confidentiality</strong>, <strong>Integrity</strong>, and <strong>Availability</strong>.</p><p><strong>Confidentiality</strong> attacks attack the privacy of data in the system, so for example stealing data, or leaking data.</p><p><strong>Integrity</strong> attacks attack the integrity of data and the system itself, which can maliciously change the behavior of the system. For example, spoofing or man in the middle attacks.</p><p><strong>Availability</strong> attacks affect the accessibility or the <em>availability</em> of the system itself. For example, Denial of Service.</p><p>If you’ve ever seen the OWASP Top 10, that’s a framework that maps these high level concepts down to specific vulnerabilities that manifest in applications. Unfortunately this framework doesn’t encompass vulnerabilities in Machine Learning systems, but luckily we&rsquo;ll be talking through those in the next few sections.</p><h3 id=how-does-application-security-apply-to-machine-learning>How does Application Security apply to Machine Learning?</h3><p>Well, Machine learning systems are applications like all other software. As such, they&rsquo;re also still vulnerable to the same CIA triad that was mentioned earlier, however in some slightly different ways.</p><table><thead><tr><th>Similar</th><th>Different</th></tr></thead><tbody><tr><td>ML Infrastructure is built on top of normal code</td><td>Machine Learning models tend to be black boxes with often non deterministic behavior</td></tr><tr><td>DoS, Supply Chain issues also still manifest</td><td>Lots of side channel information is disclosed.</td></tr></tbody></table><p>Since ML systems are built with software the same way that other applications are, they&rsquo;re vulnerable to all of the same classes of attacks that other applications are. These include business logic vulnerabilities in the code surrounding the model, DoS attacks, and supply chain attacks for whatever dependencies might be used to run the model.</p><p>However, where things become more interesting are in the model code itself. As this code is non deterministic, and often a black box, it can be difficult to reason about the security of the model itself. This is where adversarial AI comes in. By the nature of these systems, they tend to leak a lot of information about the model and it&rsquo;s internal state, which can be used to attack the model itself.</p><h3 id=confidentiality-attacks>Confidentiality Attacks</h3><p>The following attacks impact the confidentiality of the system. These include the privacy of the data used to train the model, and the privacy of the model itself.</p><h4 id=model-inversion>Model Inversion</h4><p>Model Inversion attacks involve an attacker trying to reconstruct the training data used in the construction of the model. This is done by querying the model multiple times, and using the output of the model to reconstruct the training data. This is possible because the model leaks information about the training data through its output.</p><p><img src=./images/model-inversion.jpeg alt="Model Inversion Attack"></p><p>In the above image, on the right we see an image that was used to train the machine learning model. On the left, we can see an image that was constructed by an attacker who simply queried the model multiple times, and used the output of the model (a confidence score) to reconstruct the training data.</p><p>Other examples of this attack my include attackers using Copilot to mine personal access tokens from GitHub, or ChatGPT outputting personally identifiable information from users that were in its training dataset.</p><h5 id=how-to-fix>How to Fix?</h5><p>Fixing this issue is very difficult, as side channel information disclosure is often a fundamental property of most modern machine learning systems. However, the best way to mitigate this attack is to reduce the amount of sensitive data that a model is trained on using a practice called <a href=https://en.wikipedia.org/wiki/Differential_privacy>Differential Privacy</a>. This practice involves a variety of techniques to <em>anonymize</em> individual data points while still maintaining general trends for the model to learn from.</p><h4 id=model-stealing>Model Stealing</h4><p>Model Stealing attacks involve an attacker attempting to reconstruct the model itself. This is done by querying the model multiple times, and using the output of the model to reconstruct the model itself. This is possible because the model leaks information about its internal state through its output. The simplest way to do this is to query a model multiple times to generate a training dataset, and then simply train a new model on that dataset.</p><p>This attack is extremely damaging to the model owner, as the intellectual property of the model is usually what provides most of the business value. If an attacker can steal the model, they can use it for their own purposes, or even sell it to competitors.</p><h5 id=how-to-fix-1>How to Fix?</h5><p>Fixing this issue involves rate limiting your model very heavily. An attacker needs to query your model a lot of times in order to steal it with high fidelity, so applying appropriate rate limits can help mitigate this attack. However, this is not a perfect solution, as an attacker can simply spin up multiple instances of your model to get around the rate limit. More robust solutions to this attack are still currently being researched.</p><h3 id=integrity-attacks>Integrity Attacks</h3><p>These attacks affect the integrity of the model itself. This includes attacks that can change the behavior of the model, or cause it to make incorrect predictions.</p><h4 id=dataset-poisoning>Dataset Poisoning</h4><p>Dataset poisoning attacks involve an attacker attempting to manipulate the training data used to train the model. This is done by injecting malicious data into the training dataset, which can cause the model to learn incorrect patterns, and make incorrect predictions.</p><p>Unfortunately, this class of attack is extremely common with many large machine learning models these days, as the largest source of information to train from (the Internet) is a hostile and very adversarial place. This is especially true for models that are trained on user generated content, such as ChatGPT, or Copilot.</p><h5 id=how-to-fix-2>How to Fix?</h5><p>Fixing this issue involves ensuring that the training data used to train the model is high quality, and is not maliciously manipulated. This can be done by ensuring that the data is curated by a trusted source, and that the data is not tampered with in transit.</p><h4 id=evasion-attacks>Evasion Attacks</h4><p>Evasion attacks involve an attacker modifying inputs to a model in slight ways to cause the model to make incorrect classifications. This is distinct from dataset poisoning, as the attacker is not modifying the training data, but rather the input to the model itself (this problem manifests at training time rather than data sourcing time).</p><p><img src=./images/evasion-attack.png alt="Evasion attack"></p><p>In the above image, we start with an image of a panda bear, which our model has correctly identified with 57.7% confidence. However, after we add some adversarial noise to it using the middle image (an <strong>&ldquo;adversarial perturbation&rdquo;</strong>), our model now thinks that the input image is a gibbon with 99.3% confidence.</p><p>This problem manifests due to the fact that machine learning models have a tendency to <em>memorize</em> rather than <em>learn</em>, and can tend to overfit the training data. This means that the model is not able to generalize to new inputs, meaning that slight changes in unexpected ways may cause the model to make completely incorrect classifications.</p><h5 id=how-to-fix-3>How to Fix?</h5><p>In order to fix this issue, we need to ensure that our model is not overfitting the training data. This can be achieved by perturbing the training data to the model. For example, when we clean our data, we should also add some amount of noise to it, such as rotating the image slightly, or adding some random noise to the image. This will help the model learn to generalize to new inputs, rather than memorizing the training data.</p><h4 id=prompt-injection>Prompt Injection</h4><p>Prompt injection is a special class of vulnerability that manifests only in large language models (LLMs). This class of attack involves an attacker injecting a malicious input to an LLM that causes the model to output malicious or otherwise unexpected content.</p><p>This attack occurs because large language models are not able to distinguish between a <em>prompt</em> (crafted by an engineer or an otherwise trusted source) and an <em>input</em> (crafted by a user). This means that an attacker can craft an input that causes the model to output malicious content.</p><p><img src=./images/prompt-injection.jpeg alt="Prompt Injection"></p><p>As an example of this attack, in the above picture we can see a user crafting an input to a LLM which bypasses the restrictions set by the prompt (to only respond about remote jobs) to instead output a malicious response (a threat against a public figure).</p><h5 id=how-to-fix-4>How to Fix?</h5><p>Unfortunately, as of the time of writing, there are no known ways to prevent prompt injection attacks. As such, it is important to ensure that your usage of LLMs appropriately considers this attack vector as a credible threat</p><h3 id=takeaways>Takeaways</h3><p>So what can we do to protect against these kinds of attacks? This is currently an active area of research, but there are a few general principles that should always be followed when building secure machine learning systems.</p><ol><li>Monitor your models in production.<ul><li>This is important for a number of reasons, but it allows you to detect when your model is being attacked, and allows you to take action to mitigate the attack.</li><li>You should be monitoring (i) who&rsquo;s querying your model; (ii) how often your model is being queried; (iii) what data is being sent to your model; (iv) what predictions your model is making.</li></ul></li><li>Rate limit your models.<ul><li>This is important to prevent DoS attacks, and also helps mitigate model stealing attacks.</li></ul></li><li>Train your model with robust, curated data.<ul><li>Be very cognizant of the data that you&rsquo;re using to train your model. Ensure that it is (a) high quality, and contains vetted information from non malicious users, and (b) that you have applied differential privacy to your dataset to ensure that personal information is not used to train your models.</li><li>This is an incredibly difficult step to get right, as modern models are typically trained with terabytes of data sourced from the Internet (a very adversarial environment). However, we can apply best effort data cleaning (regexes to remove PII, etc.) on this data to ensure that it is at least as private as possible.</li></ul></li><li>Implement other supplementary security controls<ul><li>Ensure that your models are only accessible by authorized users, and that dependencies are up to date.</li><li>Remember, ML Systems are just normal code, so secure it as you would anything else!</li></ul></li></ol><h3 id=further-reading>Further Reading</h3><p>For more information, check out my <a href=https://github.com/rzhade3/adversarial-ai-reading-list>Adversarial AI Reading List repository</a> on GitHub. It contains a list of papers, blog posts, and other resources that I&rsquo;ve found useful when learning about adversarial AI.</p></div></article><footer class=py:24><div class="f:fade-30 f:14 mb:8"></div><div class="f:fade-60 f:12">Theme <a class=f:bold href=https://github.com/serkodev/holy _target=_blank>Holy</a></div></footer></div></div></body></html>