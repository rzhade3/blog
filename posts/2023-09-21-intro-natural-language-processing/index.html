<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Introduction to Natural Language Processing - Blog</title><meta name=description content="A quick intro to the basic terminology and concepts in Natural Language Processing.
Token A token is the smallest semantic subsection of a piece of text. For example, a word.
This is the smallest subdivision of a text that can be assigned meaning (any smaller units than a word, like individual letters, have no inherent meaning to them)
n- gram An n- gram is a collection of n tokens. For example, if we’re using words as the gram and using the value 2 as n, in the sentence:"><meta name=author content="Rahul Zhade"><link href=https://unpkg.com/@master/normal.css rel=stylesheet><script src=https://unpkg.com/@master/style@1.5.0></script>
<script src=https://unpkg.com/@master/styles@1.13.0></script>
<script src=https://unpkg.com/master-styles-group></script>
<script src=https://unpkg.com/themes.js></script>
<script>window.themes=window.themes||new window.Themes</script><style>:root{--font-sans:"Inter var", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji}</style></head><body class="bg:fade-84@dark font:fade-16@dark font:sans"><nav class="w:full h:90 fixed bg:fade-84/.95@dark bg:white z:1000"><div class="h:full
w:full
max-w:1200
mx:auto
px:32
d:flex
align-items:center"><div><a href=/ class="mr-3 font:extralight">Blog</a></div><div class=ml:auto></div></div></nav><div class="d:flex flex:column@<=sm pt:90 px:24 jc:center gap:44 word-break:break-word"><div class="max-w:700 w:full box:content-box"><article class="box:border-box pt:32"><header class=mb:32><div class="font:40 font:extrabold">Introduction to Natural Language Processing</div><div class="mt:16 f:fade-60"><time>Sep 21, 2023</time></div></header><div class="_:where(a):hover{text-decoration-color:fade}
_:where(a){text-decoration:2;underline;fade-10;_text-decoration-color:fade-70@dark}
_:where(blockquote){bl:5;solid;fade-76/.1;_bl:5;solid;fade-34/.1@dark}
_:where(code){font:90%;_v:middle}
_:where(code:not(.highlight_*,pre_*)){p:2;6;_r:4}
_:where(del){text-decoration:1;line-through;fade-68;_text-decoration-color:red-64@dark}
_:where(figcaption){text:14;_p:10;20;0;_width:fit;_mx:auto;_font:fade-56;_font:fade-57@dark}
_:where(h1){font:40;_font:extrabold}
_:where(h1,h2,h3)+:where(h1,h2,h3){mt:.5em}
_:where(h1,h2,h3,h4,h5,h6){mt:2em}
_:where(h2){mb:1em;_font:32}
_:where(h3){font:24}
_:where(h4){font:20}
_:where(h5){font:16}
_:where(h6){font:14}
_:where(li)::marker{font:fade-44;_font:fade-68@dark}
_:where(li){pl:.375em}
_:where(mark){text-decoration:1;underline;#fce016;_bg:transparent;_text-decoration-color:rgb(252;224;22/.5)@dark}
_:where(p,li){font:fade-76;_font:16;_line-height:1.65;_font:fade-34@dark}
_:where(p,pre,blockquote,figure,ul,ol,table){my:1.125em}
>:first-child{mt:0!}
_:where(pre){p:20;_r:8;_overflow:auto}
_:where(pre,code:not(.highlight_*)){bg:fade-2;_bg:fade-92!@dark}
_:where(strong,b,a,code:not(.highlight_*),mark,del){font:fade-92;_font:fade-12@dark}
_:where(table){width:full;_border-spacing:0}
_:where(td){v:baseline}
_:where(td,th):first-child{pl:0}
_:where(td,th):last-child{pr:0}
_:where(td,th){bb:1;solid;fade-92/.06;_p:6;_b:fade-4/.04@dark}
_:where(th){font:fade-78;_font:14;_text:left;_font:fade-12@dark}
_:where(th,p_code,li_code,a,mark){font:semibold;_font:medium@dark}
_:where(ul){list-style-type:disc}
_:where(ul,ol,blockquote){pl:1.5em}
_:where(video,img){max-width:full}
_:where(a,mark){text-underline-offset:3}
_:where(hr){h:2;_bg:fade-10;_bg:fade-70@dark;_my:3em}"><p>A quick intro to the basic terminology and concepts in Natural Language Processing.</p><h3 id=token>Token</h3><p>A <strong>token</strong> is the smallest semantic subsection of a piece of text. For example, a word.</p><p>This is the smallest subdivision of a text that can be assigned meaning (any smaller units than a word, like individual letters, have no inherent meaning to them)</p><h3 id=n--gram>n- gram</h3><p>An <strong>n- gram</strong> is a collection of <code>n</code> tokens. For example, if we’re using words as the gram and using the value 2 as <code>n</code>, in the sentence:</p><blockquote><p>The quick brown fox</p></blockquote><p>The following 2-grams can be extracted:</p><pre tabindex=0><code>[the, quick],
[quick, brown],
[brown, fox]
</code></pre><p>The significance of an <strong>n- gram</strong> is that it can help model language much better than individual tokens. While tokens have meanings on their own, typically they have modifiers around them that will <em>add</em> meaning to the phrase. For example, while <em>fox</em> has inherent value, the 2- gram <em>brown fox</em> adds more information about the fox. Successively enlarging the context (length of <em>n</em>) can improve how well we’ve understood the source text, however, comes at the cost of increasing the number of n-grams that need to be processed.</p><h3 id=embeddings>Embeddings</h3><p><strong>Embeddings</strong> are a way to represent a word as a vector, to make it machine understandable.</p><p>These embedding vectors usually have a large number of dimensions. This allows words to be clustered together in multidimensional space <em>in order of similarity</em>.</p><p>I.e. words that mean similar things, like <em>like</em> and <em>enjoy</em> will sit close to one another in the embedding space.</p><p>An interesting property of these embeddings is that we can also do <em>vector math</em> with words. For example, given a modern embedding, one could perform this equation:</p><pre tabindex=0><code>&#34;king&#34; - &#34;man&#34; + &#34;woman&#34;
</code></pre><p>To get the output “queen”.</p><p>The significance of an embedding is that it allows us to encapsulate the <em>meaning</em> of words in multidimensional space, which means that we can perform similarity searches to other words, sentences, and texts with much higher performance than traditional techniques, like bag of words.</p></div></article><footer class=py:24><div class="f:fade-30 f:14 mb:8"></div><div class="f:fade-60 f:12">Theme <a class=f:bold href=https://github.com/serkodev/holy _target=_blank>Holy</a></div></footer></div></div></body></html>